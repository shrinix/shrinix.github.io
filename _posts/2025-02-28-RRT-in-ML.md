---
title: 'Reisz representation theorem and its use in Machine Learning'
date: 2025-02-28
permalink: /posts/2025/02/28/RRT-in-ML/
tags:
  - cool posts
  - category1
  - category2
---

Reisz representation theorem and its use in Machine Learning

Let R be the set of real numbers R comprised of all integers (0, -ve and
positive), rational (fractions) and irrational numbers (pi etc)

And C be the set of complex numbers of the type x ± iy where x,y are
real numbers and i is the imaginary number $\sqrt{- 1}$

Linear spaces:

Let Rn and Cn be n-dimensional linear spaces, where
$n\  \in \ \mathbb{I,\ }n > 0$. If x $\in$ Rn, then x can be written as
(x1, x2, . . . , xn), where each xi $\in R$ is called a component of x.
Similarly, if y $\in$ Cn , then y = (y1, y2, . . . , yn), where each yi
$\in C$ is a complex component of y. For the remaining discussion, to
simplify things, we will consider only Rn­­.

Norm: In an n-dimensional linear vector space V based on real numbers,
R, the norm represents the lengths of the vectors of the space. A norm
on V is a function ∥v∥ : V → R.

In particular, the norm on R^2^ (also known as Euclidean norm) is
defined as:
$\left\| \left( x_{1},x_{2} \right) \right\|\ : = \sqrt{{x_{1}}^{2} + {x_{2}}^{2}}$

Inner product: In an n-dimensional linear space V based on real numbers,
R, an inner product is defined as a function from V to R.
$\left\langle x \middle| y \right\rangle\ : = \ $V x V -\> R, such that:

$\left\langle x \middle| y \right\rangle\  = \sum_{k = 1}^{n}{\binom{n}{k}x^{k}y^{k}}$,
where x = (x1, x2, . . . , xn), and y = (y1, y2, . . . , yn) are vectors
in V

The inner product of two vectors in a linear vector space, if it exists,
represents the planar angle between the vectors.

A vector space V with an inner product
$\left\langle x \middle| y \right\rangle\ $where x, y $\in V\ $has an
induced norm given by: $\sqrt{\left\langle x \middle| y \right\rangle}$

If the linear spaces can be extended to infinite dimensions, we can use
them to represent and analyze functions. A function can be treated as a
vector with infinitely many entries or a vector in an
infinite-dimensional space.

If f and g are real-valued functions in an infinite-dimensional linear
space X of functions defined in the real interval \[a,b\], then, the
inner product in X is defined as: $(f,g)\ : = \int_{a}^{b}{f.g}$

Linear operators:

Linear map:

Banach space: A linear space with a norm is known as a Banach space.

Hilbert Spaces: A Hilbert space is a complete linear, infinite
dimensional inner-product space with an induced norm.

Orthogonal vectors: Two vectors x and y in a Hilbert space H are
orthogonal if their inner product is 0 i.e. \<x\|y\> = 0.

Orthonormal vectors:

Orthogonal decomposition: If M is a closed subspace of Hilbert space H,
then we can write: H=M⊕M^⊥^, where ⊕ is the direct sum operator, and
M^⊥^ = {x∈H :⟨x,y⟩=0,y∈M} is the subspace containing elements that are
perpendicular to elements of M. Any x ∈ H, can be expressed uniquely as
x = y + z where y ∈ M and z ∈ M^⊥^. Further, it can be shown that y and
z are the unique elements of M and M^⊥^ whose distance to x is smallest.
If (a,b) and (c,d) are ordered pairs, their direct sum is defined as:
(a,b)+(c,d) = (a + c, b + d).

Linear Functional: A functional L is a function or a linear
transformation that maps an n-dimensional vector space V = Rn to a real
number R. L := Rn -\> R. A linear map V → K where K is a one-dimensional
vector space is called a linear functional. Here, V is the domain and K
is the Image space

Kernel or Nullspace: The kernel of a linear map is the linear subspace
of the domain of the map which is mapped to the zero vector. The kernel
is also known as the nullspace of the domain.

Dimension Theorem: In the finite-dimensional case, if L is a linear map:
L: V-\>W, where V and W are finite-dimensional vector spaces, the
dimension theorem is given by: dim (V) = dim (Ker(L)) + dim(Im(L))

  -----------------------------------------------------------------------
  ![](media/image1.png){width="3.0569444444444445in"
  height="2.0569444444444445in"}
  -----------------------------------------------------------------------
  Source: Wikipedia

  -----------------------------------------------------------------------

Dual-space of Hilbert Space: Every vector space has its dual. The space
H^\*^ of all continuous linear functionals is called the dual-space of
H.

If L is defined as the inner-product of H, i.e. L := H x H -\> R*,* then
the subspace ker(L) = {x : L(x) = 0} is the kernel or null space of L.
If u, v $\in H$ , then ker(L) = \<u,v\> = 0 and it consists of vectors v
which are perpendicular to u.

Since L is a linear functional in H, ker(L) is a dual-space of H.

Riesz Representation Theorem (RRT): If H is a Hilbert space with
an[y]{.underline} inner product \< \* . \*\> := H x H -\> R and L is a
bounded, continuous linear functional, L := H -\> R, then, for every L
$\in H$^\*^, where H^\*^ is the dual-space of H, there exists a unique
element u $\in H$ such that L(v) = \<u,v\> $\forall\ v\  \in H$. Since u
is unique, it "represents" the linear functional L.

If we apply RRT to Ker(L), which is a subspace of H, then, we get L(v) =
\<u,v\> = 0 and we can find the unique vector u in Ker(L) which is also
a dual-space of H.

+-----------------------------------------------------------------------+
| Ker(L)                                                                |
|                                                                       |
| e~1~ = (1,0,0)                                                        |
|                                                                       |
| e~3~ = (0,0,1)                                                        |
|                                                                       |
| e~2~ = (0,1,0)                                                        |
|                                                                       |
| **R^3^**                                                              |
|                                                                       |
| u                                                                     |
|                                                                       |
| v                                                                     |
|                                                                       |
| L: R^3^ -\> R^1^                                                      |
|                                                                       |
| R^1^                                                                  |
|                                                                       |
| 0                                                                     |
+=======================================================================+
|                                                                       |
+-----------------------------------------------------------------------+

Using the dimension theorem, dim (V) = dim (Ker(L)) + dim(Im(L)),
dim(Im(L)) = dim (R^1^) = 1 and dim(V) = dim(H^3^) = 3 =\> dim(Ker(L)) =
3-1 = 2.

If L is a linear functional defined as follows:

L(e­~1~) = 2, L(e­~2~) = 1, L(e­~2~) = 1

Then, if u = (2,1,1), then,

e­~1~ = (1,0,0), e­~1~ = (0,1,0), e­~1~ = (0,0,1)

L(e­~1~) = \<u. e­~1~\> = u~1~­­. e­~11~ + u~2~. e­~12\ +~ u~3~. e­~13~ =
2.1+1.0+1.0 = 2

L(e­~2~) = \<u. e­~2~\> = u~1~. e­~21~ + u~2~. e­~22\ +~ u­~3~. e­~23~ =
3.0+1.1+1.0 = 1

L(e­~3~) = \<u. e­~3~\> = u~1~. e­~31~ + u~2~. e­~22\ +~ u~3~. e­~23~ =
3.0+1.0+1.1 = 1

Let, v = (0,-1,1) $\in$ Ker(L). From the dimension theorem, Ker(L) is a
2-dimensional plane.

L(v) = \<u,v\> = u~1~­­. v~1~ + u~2~. v­~2\ +~ u~3~. v~3~ = 2.0 -1.1+1.1 =
0

Similarly, we can prove that for any vector z $\in$ Ker(L), L(z) =
\<u,z\> = 0, since z $\equiv \ $(z~1~, z~2~, z~3~), where z~1~ = 0 and
z~3~ = - z~2~ ^.^Thus, in lower dimensions, the unique vector u which
"represents" the linear functional L is orthogonal / normal to every
vector in the plane represented by the kernel of L. So, u ∈ ker(L)⊥­.

It is possible to prove, for any number of dimensions (see \[1\] below),
that kernel or null space of L coincides with the orthogonal space of u
where, the orthogonal space of u is given by: u⊥ ={x∈Rn :x·u=0}.
